{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_sIc_vI83CW5"
   },
   "source": [
    "## Obteniendo todos los tweets de un usuario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "zg5BuU9a3gy7"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "import tweepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "ZY8WyH504C0H"
   },
   "outputs": [],
   "source": [
    "#pip install --upgrade tweepy  # needed v.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "k4Tv2ESo2BIw"
   },
   "outputs": [],
   "source": [
    "# LA LLAVE\n",
    "BEARER_TOKEN = \"AAAAAAAAAAAAAAAAAAAAAF02cQEAAAAAAw4Ko%2BlvaP%2FEKo4jetBjHN%2BGEpY%3D8ot7Zeg7DytI7xLRXuZO23cSWSLBIAHPMKrw9b8jebi9j7nWsI\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ubalv7VSMNkO"
   },
   "source": [
    "#Extracción de data a partir del @username.\n",
    "Basado en Tweepy v4.9.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "ZV7UJqdJ5Eid"
   },
   "outputs": [],
   "source": [
    "input_user = 'mferna' #Aquí se coloca el usuario al que se quiere revisar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "06b-kj461hfr"
   },
   "outputs": [],
   "source": [
    "def extract_tweet_data(input_user):\n",
    "    \"\"\"\n",
    "    Extracts data from a Twitter account.\n",
    "\n",
    "    Parameters:\n",
    "    input_user: str\n",
    "        Twitter account to be analysed (without the @).\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame\n",
    "        A dataframe of the user tweets containing, time of creation,\n",
    "        text, tweet id, number of retweets and number of likes.\n",
    "    \"\"\"\n",
    "    # Authorize access\n",
    "    client = tweepy.Client(BEARER_TOKEN)\n",
    "\n",
    "    # Get user ID from username\n",
    "    user_id = client.get_user(username=input_user).data.id\n",
    "\n",
    "    # Get Tweets timeline (check all pages, max 100 tweets per page, max 32 pages)\n",
    "    df_list = []\n",
    "    df_len0 = []\n",
    "    df_len = []\n",
    "    for response in tweepy.Paginator(client.get_users_tweets, user_id,\n",
    "                                    tweet_fields='created_at,lang,public_metrics',\n",
    "                                    max_results=100, limit=32):\n",
    "        page_df = pd.DataFrame.from_dict(response.data)\n",
    "        df_len0.append(len(page_df))\n",
    "        # Delete non-spanish tweets\n",
    "        page_df = page_df.drop(page_df[page_df.lang !='es'].index)\n",
    "        page_df = page_df.drop(['lang'], axis=1)\n",
    "        df_len.append(len(page_df))\n",
    "        # Organize columns\n",
    "        retweet_count = []\n",
    "        like_count = []\n",
    "        for item in page_df['public_metrics'].tolist():\n",
    "          retweet_count.append(item['retweet_count'])\n",
    "          like_count.append(item['like_count'])\n",
    "        page_df = page_df.drop(['public_metrics'], axis=1)\n",
    "        page_df['retweet_count'] = retweet_count\n",
    "        page_df['like_count'] = like_count\n",
    "        df_list.append(page_df)\n",
    "    es_tweets = round(sum(df_len)*100/sum(df_len0),1)\n",
    "    print(f'Obtained {es_tweets}% tweets in spanish from total of {sum(df_len0)} tweets from user @{input_user}.')\n",
    "    tweets_df = pd.concat(df_list, ignore_index=True)\n",
    "    return tweets_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z30Yc-sFKSIz"
   },
   "source": [
    "OLD EXTRACTION CODE (only works for obtaining user info, and only 100 tweets max)\\\n",
    "Es una modificación de los ejemplos para obtener [info del user](https://github.com/twitterdev/Twitter-API-v2-sample-code/blob/main/User-Lookup/get_users_with_bearer_token.py) y [sus tweets](https://github.com/twitterdev/Twitter-API-v2-sample-code/blob/main/User-Tweet-Timeline/user_tweets.py)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "ySCOAv1-s5Je"
   },
   "outputs": [],
   "source": [
    "def create_url(username, twitter_object):\n",
    "    if twitter_object == 'user':\n",
    "      usernames = f'usernames={input_user}'\n",
    "      user_fields = 'user.fields=description,created_at,protected,public_metrics,verified'\n",
    "      url = f'https://api.twitter.com/2/users/by?{usernames}&{user_fields}'\n",
    "    elif twitter_object == 'tweet':\n",
    "      user_dict = get_json('user')\n",
    "      user_id = user_dict['data'][0]['id']\n",
    "      url = f'https://api.twitter.com/2/users/{user_id}/tweets?max_results=100'\n",
    "    else:\n",
    "      print('Error. Wrong Twitter Object.')\n",
    "      return None\n",
    "    return url\n",
    "\n",
    "\n",
    "def get_params(twitter_object):\n",
    "    if twitter_object == 'tweet':\n",
    "      params = {\"tweet.fields\": \"created_at,lang,public_metrics\"}\n",
    "    else:\n",
    "      print('Error. Wrong Twitter Object.')\n",
    "      return None\n",
    "    return params\n",
    "\n",
    "\n",
    "def bearer_oauth_user(r):\n",
    "    r.headers[\"Authorization\"] = f\"Bearer {BEARER_TOKEN}\"\n",
    "    r.headers[\"User-Agent\"] = \"v2UserLookupPython\"\n",
    "    return r\n",
    "\n",
    "\n",
    "def bearer_oauth_tweet(r):\n",
    "    r.headers[\"Authorization\"] = f\"Bearer {BEARER_TOKEN}\"\n",
    "    r.headers[\"User-Agent\"] = \"v2UserTweetsPython\"\n",
    "    return r\n",
    "\n",
    "\n",
    "def connect_to_endpoint(url, twitter_object):\n",
    "    if twitter_object == 'user':\n",
    "      response = requests.request(\"GET\", url, auth=bearer_oauth_user,)\n",
    "    elif twitter_object == 'tweet':\n",
    "      params = get_params('tweet')\n",
    "      response = requests.request(\"GET\", url, auth=bearer_oauth_tweet,params=params)\n",
    "    else:\n",
    "        print('Error. Wrong Twitter Object.')\n",
    "        return None\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(f'Request returned an error: {response.status_code} {response.text}')\n",
    "    return response.json()\n",
    "\n",
    "\n",
    "def get_json(twitter_object):\n",
    "    if twitter_object == 'user':\n",
    "      url = create_url(input_user, 'user')\n",
    "      json_response = connect_to_endpoint(url, 'user')\n",
    "    elif twitter_object == 'tweet':\n",
    "      url = create_url(input_user, 'tweet')\n",
    "      json_response = connect_to_endpoint(url, 'tweet')\n",
    "    else:\n",
    "        print('Error. Wrong Twitter Object.')\n",
    "        return None\n",
    "    return json_response\n",
    "\n",
    "\n",
    "def extract_user_data(username):  # MAIN FILE\n",
    "    \"\"\"\n",
    "    Extracts data from a Twitter account.\n",
    "\n",
    "    Parameters:\n",
    "    input_user: str\n",
    "        Twitter account to be analysed (without the @).\n",
    "    Returns:\n",
    "    dict\n",
    "        A dict containing the releveant variables about the user\n",
    "    \"\"\"\n",
    "    # Get user ID from username\n",
    "    user_dict = get_json('user')\n",
    "    #user_id = user_dict['data'][0]['id']\n",
    "\n",
    "    # Get Tweets timeline\n",
    "    #tweet_dict = get_json('tweet')\n",
    "    #tweet_pd = pd.DataFrame.from_dict(tweet_dict['data'])\n",
    "    return user_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jzybELSiTdWI"
   },
   "source": [
    "#Información del usuario extraida del API de twitter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QUt6St5mOAxL",
    "outputId": "491640fa-8607-4830-c8f0-7d74ca2983ad"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': [{'description': '2das NUPCIAS CON LA PAJARRACA TWITTER. \\nContra la CABAL que corrompió la Casta Política, \\nsecuestró la Medicina, al Periodismo y Libertades Individuales',\n",
       "   'name': 'NACHINO _OFICIAL',\n",
       "   'protected': False,\n",
       "   'created_at': '2018-04-28T16:09:33.000Z',\n",
       "   'public_metrics': {'followers_count': 7247,\n",
       "    'following_count': 7114,\n",
       "    'tweet_count': 43332,\n",
       "    'listed_count': 21},\n",
       "   'verified': False,\n",
       "   'username': 'ignaziololo1',\n",
       "   'id': '990261747557814272'}]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_user_data(input_user)  ## Información del usuario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 287
    },
    "id": "ffWCJ56kOoLk",
    "outputId": "2146db50-851a-430a-9aa6-f38ea8546feb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtained 79.5% tweets in spanish from total of 3161 tweets from user @ignaziololo1.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_at</th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>like_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-05-08 15:30:08+00:00</td>\n",
       "      <td>1523324379807072258</td>\n",
       "      <td>RT @hip0critic: @FrayJosepho \"COBIPERSISTENTE\"...</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-05-08 14:31:10+00:00</td>\n",
       "      <td>1523309540137127937</td>\n",
       "      <td>RT @BassanVirginia: @sembraodo2 @ignaziololo1 ...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-05-08 14:24:59+00:00</td>\n",
       "      <td>1523307982087368706</td>\n",
       "      <td>Buen Domingo y descanso a todos. \\nRock Puro d...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-05-08 14:17:53+00:00</td>\n",
       "      <td>1523306196689891329</td>\n",
       "      <td>RT @Pablo86418579: https://t.co/xdVHT8Mtwt\\nAc...</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-05-08 14:17:39+00:00</td>\n",
       "      <td>1523306139282448384</td>\n",
       "      <td>RT @sembraodo2: @ignaziololo1 terribles jesuit...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2509</th>\n",
       "      <td>2022-03-15 14:21:56+00:00</td>\n",
       "      <td>1503738269355454468</td>\n",
       "      <td>RT @_RcaArg: @ignaziololo1 @bdmgem Quizás esté...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2510</th>\n",
       "      <td>2022-03-15 14:01:35+00:00</td>\n",
       "      <td>1503733147611107329</td>\n",
       "      <td>Victoria Nuland fue la que diseñó el golpe de ...</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2511</th>\n",
       "      <td>2022-03-15 13:50:11+00:00</td>\n",
       "      <td>1503730278703017984</td>\n",
       "      <td>@_RcaArg @Alberto65813877 Vale.</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2512</th>\n",
       "      <td>2022-03-15 13:48:45+00:00</td>\n",
       "      <td>1503729920253566981</td>\n",
       "      <td>@GretaDiLuna Facturaron una reforma que no se ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2513</th>\n",
       "      <td>2022-03-15 13:38:46+00:00</td>\n",
       "      <td>1503727406917554178</td>\n",
       "      <td>RT @San_Recargada: Esto es un HORROR!!! Es ase...</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2514 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    created_at                   id  \\\n",
       "0    2022-05-08 15:30:08+00:00  1523324379807072258   \n",
       "1    2022-05-08 14:31:10+00:00  1523309540137127937   \n",
       "2    2022-05-08 14:24:59+00:00  1523307982087368706   \n",
       "3    2022-05-08 14:17:53+00:00  1523306196689891329   \n",
       "4    2022-05-08 14:17:39+00:00  1523306139282448384   \n",
       "...                        ...                  ...   \n",
       "2509 2022-03-15 14:21:56+00:00  1503738269355454468   \n",
       "2510 2022-03-15 14:01:35+00:00  1503733147611107329   \n",
       "2511 2022-03-15 13:50:11+00:00  1503730278703017984   \n",
       "2512 2022-03-15 13:48:45+00:00  1503729920253566981   \n",
       "2513 2022-03-15 13:38:46+00:00  1503727406917554178   \n",
       "\n",
       "                                                   text  retweet_count  \\\n",
       "0     RT @hip0critic: @FrayJosepho \"COBIPERSISTENTE\"...             13   \n",
       "1     RT @BassanVirginia: @sembraodo2 @ignaziololo1 ...              2   \n",
       "2     Buen Domingo y descanso a todos. \\nRock Puro d...              0   \n",
       "3     RT @Pablo86418579: https://t.co/xdVHT8Mtwt\\nAc...              5   \n",
       "4     RT @sembraodo2: @ignaziololo1 terribles jesuit...              1   \n",
       "...                                                 ...            ...   \n",
       "2509  RT @_RcaArg: @ignaziololo1 @bdmgem Quizás esté...              3   \n",
       "2510  Victoria Nuland fue la que diseñó el golpe de ...             10   \n",
       "2511                    @_RcaArg @Alberto65813877 Vale.              0   \n",
       "2512  @GretaDiLuna Facturaron una reforma que no se ...              0   \n",
       "2513  RT @San_Recargada: Esto es un HORROR!!! Es ase...             16   \n",
       "\n",
       "      like_count  \n",
       "0              0  \n",
       "1              0  \n",
       "2              1  \n",
       "3              0  \n",
       "4              0  \n",
       "...          ...  \n",
       "2509           0  \n",
       "2510           9  \n",
       "2511           1  \n",
       "2512           1  \n",
       "2513           0  \n",
       "\n",
       "[2514 rows x 5 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_tweet_data(input_user)  ## DataFrame con todos los tweets en español del usuario"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PaYmSBmS5Wb0"
   },
   "source": [
    "### Procesamiento de data\n",
    "\n",
    "Limpieza y lematizacion de texto\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "C1yYEvhi8wsk"
   },
   "outputs": [],
   "source": [
    "def vars_word(word, input_df):\n",
    "    \"\"\"\n",
    "    Compute the variables for a given word.\n",
    "\n",
    "    Parameters:\n",
    "    word: str\n",
    "        Word to be analysed in the timeline tweets.\n",
    "    input_df: pd.DataFrame\n",
    "        A dataframe of the user tweets containing, time of creation,\n",
    "        text, tweet id, number of retweets and number of likes.\n",
    "\n",
    "    Returns:\n",
    "    tuple(float, float, float, float, float):\n",
    "        A tuple containing variables of the word: frequency, like rate, retweet\n",
    "        rate, popularity, and polemicity\n",
    "    tuple(float, float, float):\n",
    "        A tuple containing counters of the word: number of times it appears\n",
    "        on all the tweets, number of tweets it appears in, number of retweets it\n",
    "        appears in.\n",
    "    \"\"\"\n",
    "\n",
    "    df = input_df.copy()\n",
    "    # Flatten text column\n",
    "    splitted_tweets=df.clean_text.apply(lambda a: a.split()).tolist()#.count(\"datos\")\n",
    "    # Flatten text column\n",
    "    total_word_list = [word_tweet for word_list in splitted_tweets for word_tweet in word_list]\n",
    "    \n",
    "\n",
    "    word_count = total_word_list.count(word)\n",
    "    total_word_count = len(total_word_list)\n",
    "    freq = word_count/total_word_count\n",
    "\n",
    "    df['word_count'] = df.clean_text.map(lambda t: t.count(word))\n",
    "    tweets_containing = df['word_count'].astype(bool).sum()\n",
    "    #if tweets_containing == 0:\n",
    "    #    return None\n",
    "    df.drop(['word_count'], axis=1)\n",
    "    # like_count_flag\n",
    "    like_count = df[df['word_count'].astype(bool)]['like_count'].sum()\n",
    "    retweet_count = df[df['word_count'].astype(bool)]['retweet_count'].sum()\n",
    "    #if (like_count == 0) or (retweet_count == 0):\n",
    "    #    return None\n",
    "    like_rate = like_count/tweets_containing\n",
    "    retweet_rate = retweet_count/tweets_containing\n",
    "\n",
    "    popularity  = retweet_rate/freq\n",
    "    polemicity  = retweet_rate/like_rate\n",
    "\n",
    "    variables = (freq, like_rate, retweet_rate, popularity, polemicity)\n",
    "    counters = (word_count, like_count, retweet_count)\n",
    "    return variables, counters\n",
    "\n",
    "\n",
    "def vars_cat(category, df):\n",
    "    \"\"\"\n",
    "    Compute the variables for a given category.\n",
    "\n",
    "    Parameters:\n",
    "    category: list\n",
    "        List of words to be analysed in group in the timeline tweets.\n",
    "    df: pd.DataFrame\n",
    "        A dataframe of the user tweets containing, time of creation,\n",
    "        text, tweet id, number of retweets and number of likes.\n",
    "\n",
    "    Returns:\n",
    "    tuple(float, float, float):\n",
    "        A tuple containing counters of the words in the category: number of\n",
    "        times they appear on all the tweets, number of tweets they appears in,\n",
    "        number of retweets they appear in.\n",
    "    tuple(float, float, float):\n",
    "        A tuple containing popularity and polemicity of the category.\n",
    "    \"\"\"\n",
    "\n",
    "    category = [word for word in category if word]\n",
    "    pop_cat = pol_cat = word_count_cat = like_count_cat = retweet_count_cat = 0\n",
    "    for word in category:\n",
    "        #if vars_word(word, df) is None:\n",
    "        vars, counters = vars_word(word, df)\n",
    "        pop_cat += counters[0] * vars[3]\n",
    "        pol_cat += counters[0] * vars[4]\n",
    "        word_count_cat += counters[0]\n",
    "        like_count_cat += counters[1]\n",
    "        retweet_count_cat += counters[2]\n",
    "\n",
    "    pop_cat = pop_cat / word_count_cat\n",
    "    pol_cat = pol_cat / word_count_cat\n",
    "\n",
    "    cat_counters = (word_count_cat, like_count_cat, retweet_count_cat)\n",
    "    pop_pol_cats = (pop_cat, pol_cat)\n",
    "    return cat_counters, pop_pol_cats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PGZeprWKIO0m"
   },
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "zEuHzoym814S"
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "9Y6zxw06814T"
   },
   "outputs": [],
   "source": [
    "# Load spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def clean_string(text, stem=\"None\"):\n",
    "    try:\n",
    "\n",
    "        final_string = \"\"\n",
    "\n",
    "        # minusculas\n",
    "        text = text.lower()\n",
    "        text = text.replace(\" u \",\" \").replace(\"á\",\"a\").replace(\"é\",\"e\").replace(\"í\",\"i\").replace(\"ó\",\"o\").replace(\"ú\",\"u\")\n",
    "\n",
    "        # quitar salto de lineas\n",
    "        emoji_pattern = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "        u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "        u\"\\U0001f926-\\U0001f937\"\n",
    "        u\"\\U00010000-\\U0010ffff\"\n",
    "        u\"\\u2640-\\u2642\" \n",
    "        u\"\\u2600-\\u2B55\"\n",
    "        u\"\\u200d\"\n",
    "        u\"\\u23cf\"\n",
    "        u\"\\u23e9\"\n",
    "        u\"\\u231a\"\n",
    "        u\"\\ufe0f\"  # dingbats\n",
    "        u\"\\u3030\"\n",
    "                      \"]+\", re.UNICODE)\n",
    "        text=emoji_pattern.sub(r'', text)\n",
    "        # quitar signos gramaticales\n",
    "        translator = str.maketrans('', '', string.punctuation)\n",
    "        text = text.translate(translator)\n",
    "\n",
    "        # stopwords\n",
    "        text = text.split()\n",
    "        useless_words = nltk.corpus.stopwords.words(\"spanish\")\n",
    "        useless_words = useless_words + ['...','considero','tal','vez','seria','debe','tener','siento', \"dar\", \"hacer\"\n",
    "                            'estan','tan','parece','ademas','debido','cuenta','hace','cada','toda','si', \"ser\",\n",
    "                                         \"ma\", \"mas\", \"más\",\"bien\", \"buena\", \"creo\", \"aun\"]\n",
    "        # \n",
    "        text_filtered = [word for word in text if not word in useless_words]\n",
    "\n",
    "        # quitar numeros\n",
    "        text_filtered = [re.sub(r'\\w*\\d\\w*', '', w) for w in text_filtered]\n",
    "        \n",
    "\n",
    "        # Lematizacion\n",
    "        #if stem == 'Stem':\n",
    "        #    stemmer = PorterStemmer() \n",
    "        #    text_stemmed = [stemmer.stem(y) for y in text_filtered]\n",
    "        #elif stem == 'Lem':\n",
    "        #    lem = WordNetLemmatizer()\n",
    "        #    text_stemmed = [lem.lemmatize(y) for y in text_filtered]\n",
    "        #elif stem == 'Spacy':\n",
    "        #    text_filtered = nlp(' '.join(text_filtered))\n",
    "        #    text_stemmed = [y.lemma_ for y in text_filtered]\n",
    "        #else:\n",
    "        #    text_stemmed = text_filtered\n",
    "\n",
    "        #final_string = ' '.join(text_stemmed)\n",
    "        final_string = ' '.join(text_filtered)\n",
    "\n",
    "        # sinonimia\n",
    "        #final_string = final_string.replace('docent','profesor')\n",
    "    except AttributeError:\n",
    "        final_string = \"\"\n",
    "\n",
    "    return final_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "noSa2PuL814U"
   },
   "outputs": [],
   "source": [
    "def ngrams_wrapper_1(sent):\n",
    "    return list(nltk.ngrams(sent, 1))\n",
    "\n",
    "def ngrams_wrapper_2(sent):\n",
    "    return list(nltk.ngrams(sent, 2))\n",
    "\n",
    "def ngrams_wrapper_3(sent):\n",
    "    return list(nltk.ngrams(sent, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "NiYtEQ4M814V"
   },
   "outputs": [],
   "source": [
    "def make_list_bigrams_1(text):\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    tokenized = map(nltk.tokenize.word_tokenize, sentences)\n",
    "    bigrams = map(ngrams_wrapper_1, tokenized)\n",
    "    bigram = list(itertools.chain.from_iterable(bigrams))\n",
    "    return bigram\n",
    "\n",
    "def make_list_bigrams_2(text):\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    tokenized = map(nltk.tokenize.word_tokenize, sentences)\n",
    "    bigrams = map(ngrams_wrapper_2, tokenized)\n",
    "    bigram = list(itertools.chain.from_iterable(bigrams))\n",
    "    return bigram\n",
    "\n",
    "def make_list_bigrams_3(text):\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    tokenized = map(nltk.tokenize.word_tokenize, sentences)\n",
    "    bigrams = map(ngrams_wrapper_3, tokenized)\n",
    "    bigram = list(itertools.chain.from_iterable(bigrams))\n",
    "    return bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "ptXVujDQ814W"
   },
   "outputs": [],
   "source": [
    "def nlp_process_1(text):\n",
    "    return '/'.join([str(i) for i in make_list_bigrams_1(clean_string(str(text), stem='Stem'))])\n",
    "\n",
    "def nlp_process_2(text):\n",
    "    return '/'.join([str(i) for i in make_list_bigrams_2(clean_string(str(text), stem='Stem'))])\n",
    "\n",
    "def nlp_process_3(text):\n",
    "    return '/'.join([str(i) for i in make_list_bigrams_3(clean_string(str(text), stem='Stem'))])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "622fc216"
   },
   "source": [
    "# Subida de los tweets del usuario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "i6F1GAme814W"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtained 79.5% tweets in spanish from total of 3161 tweets from user @ignaziololo1.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_at</th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>like_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-05-08 15:30:08+00:00</td>\n",
       "      <td>1523324379807072258</td>\n",
       "      <td>RT @hip0critic: @FrayJosepho \"COBIPERSISTENTE\"...</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-05-08 14:31:10+00:00</td>\n",
       "      <td>1523309540137127937</td>\n",
       "      <td>RT @BassanVirginia: @sembraodo2 @ignaziololo1 ...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-05-08 14:24:59+00:00</td>\n",
       "      <td>1523307982087368706</td>\n",
       "      <td>Buen Domingo y descanso a todos. \\nRock Puro d...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-05-08 14:17:53+00:00</td>\n",
       "      <td>1523306196689891329</td>\n",
       "      <td>RT @Pablo86418579: https://t.co/xdVHT8Mtwt\\nAc...</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-05-08 14:17:39+00:00</td>\n",
       "      <td>1523306139282448384</td>\n",
       "      <td>RT @sembraodo2: @ignaziololo1 terribles jesuit...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2509</th>\n",
       "      <td>2022-03-15 14:21:56+00:00</td>\n",
       "      <td>1503738269355454468</td>\n",
       "      <td>RT @_RcaArg: @ignaziololo1 @bdmgem Quizás esté...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2510</th>\n",
       "      <td>2022-03-15 14:01:35+00:00</td>\n",
       "      <td>1503733147611107329</td>\n",
       "      <td>Victoria Nuland fue la que diseñó el golpe de ...</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2511</th>\n",
       "      <td>2022-03-15 13:50:11+00:00</td>\n",
       "      <td>1503730278703017984</td>\n",
       "      <td>@_RcaArg @Alberto65813877 Vale.</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2512</th>\n",
       "      <td>2022-03-15 13:48:45+00:00</td>\n",
       "      <td>1503729920253566981</td>\n",
       "      <td>@GretaDiLuna Facturaron una reforma que no se ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2513</th>\n",
       "      <td>2022-03-15 13:38:46+00:00</td>\n",
       "      <td>1503727406917554178</td>\n",
       "      <td>RT @San_Recargada: Esto es un HORROR!!! Es ase...</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2514 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    created_at                   id  \\\n",
       "0    2022-05-08 15:30:08+00:00  1523324379807072258   \n",
       "1    2022-05-08 14:31:10+00:00  1523309540137127937   \n",
       "2    2022-05-08 14:24:59+00:00  1523307982087368706   \n",
       "3    2022-05-08 14:17:53+00:00  1523306196689891329   \n",
       "4    2022-05-08 14:17:39+00:00  1523306139282448384   \n",
       "...                        ...                  ...   \n",
       "2509 2022-03-15 14:21:56+00:00  1503738269355454468   \n",
       "2510 2022-03-15 14:01:35+00:00  1503733147611107329   \n",
       "2511 2022-03-15 13:50:11+00:00  1503730278703017984   \n",
       "2512 2022-03-15 13:48:45+00:00  1503729920253566981   \n",
       "2513 2022-03-15 13:38:46+00:00  1503727406917554178   \n",
       "\n",
       "                                                   text  retweet_count  \\\n",
       "0     RT @hip0critic: @FrayJosepho \"COBIPERSISTENTE\"...             13   \n",
       "1     RT @BassanVirginia: @sembraodo2 @ignaziololo1 ...              2   \n",
       "2     Buen Domingo y descanso a todos. \\nRock Puro d...              0   \n",
       "3     RT @Pablo86418579: https://t.co/xdVHT8Mtwt\\nAc...              5   \n",
       "4     RT @sembraodo2: @ignaziololo1 terribles jesuit...              1   \n",
       "...                                                 ...            ...   \n",
       "2509  RT @_RcaArg: @ignaziololo1 @bdmgem Quizás esté...              3   \n",
       "2510  Victoria Nuland fue la que diseñó el golpe de ...             10   \n",
       "2511                    @_RcaArg @Alberto65813877 Vale.              0   \n",
       "2512  @GretaDiLuna Facturaron una reforma que no se ...              0   \n",
       "2513  RT @San_Recargada: Esto es un HORROR!!! Es ase...             16   \n",
       "\n",
       "      like_count  \n",
       "0              0  \n",
       "1              0  \n",
       "2              1  \n",
       "3              0  \n",
       "4              0  \n",
       "...          ...  \n",
       "2509           0  \n",
       "2510           9  \n",
       "2511           1  \n",
       "2512           1  \n",
       "2513           0  \n",
       "\n",
       "[2514 rows x 5 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dummy input (after cleaning)\n",
    "o_df = extract_tweet_data(input_user)\n",
    "#test_df = o_df.copy()\n",
    "o_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JeXTv7IJ_sUV"
   },
   "source": [
    "Limpieza de los tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "rcUFOzhC814X"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_at</th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>like_count</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>bigram_key_words</th>\n",
       "      <th>trigram_key_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-05-08</td>\n",
       "      <td>1523324379807072258</td>\n",
       "      <td>RT @hip0critic: @FrayJosepho \"COBIPERSISTENTE\"...</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>rt  frayjosepho cobipersistente dicese hackeo ...</td>\n",
       "      <td>('rt', 'frayjosepho')/('frayjosepho', 'cobiper...</td>\n",
       "      <td>('rt', 'frayjosepho', 'cobipersistente')/('fra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-05-08</td>\n",
       "      <td>1523309540137127937</td>\n",
       "      <td>RT @BassanVirginia: @sembraodo2 @ignaziololo1 ...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>rt bassanvirginia   crees papa francisco insta...</td>\n",
       "      <td>('rt', 'bassanvirginia')/('bassanvirginia', 'c...</td>\n",
       "      <td>('rt', 'bassanvirginia', 'crees')/('bassanvirg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-05-08</td>\n",
       "      <td>1523307982087368706</td>\n",
       "      <td>Buen Domingo y descanso a todos. \\nRock Puro d...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>buen domingo descanso rock puro pura sangre li...</td>\n",
       "      <td>('buen', 'domingo')/('domingo', 'descanso')/('...</td>\n",
       "      <td>('buen', 'domingo', 'descanso')/('domingo', 'd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-05-08</td>\n",
       "      <td>1523306196689891329</td>\n",
       "      <td>RT @Pablo86418579: https://t.co/xdVHT8Mtwt\\nAc...</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>rt   aca luisiana eeuu aborto entiende  uribur...</td>\n",
       "      <td>('rt', 'aca')/('aca', 'luisiana')/('luisiana',...</td>\n",
       "      <td>('rt', 'aca', 'luisiana')/('aca', 'luisiana', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-05-08</td>\n",
       "      <td>1523306139282448384</td>\n",
       "      <td>RT @sembraodo2: @ignaziololo1 terribles jesuit...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>rt   terribles jesuitas</td>\n",
       "      <td>('rt', 'terribles')/('terribles', 'jesuitas')</td>\n",
       "      <td>('rt', 'terribles', 'jesuitas')</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2022-05-08</td>\n",
       "      <td>1523305329639006208</td>\n",
       "      <td>Hablamos de la Viruela hace 2 meses... que \"de...</td>\n",
       "      <td>7</td>\n",
       "      <td>11</td>\n",
       "      <td>hablamos viruela  meses desaparecio vial viruz...</td>\n",
       "      <td>('hablamos', 'viruela')/('viruela', 'meses')/(...</td>\n",
       "      <td>('hablamos', 'viruela', 'meses')/('viruela', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2022-05-08</td>\n",
       "      <td>1523304162280108032</td>\n",
       "      <td>RT @RCLatin2: @milostris @sallelorier @SciutoD...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>rt  milostris sallelorier sciutodr    curiosa ...</td>\n",
       "      <td>('rt', 'milostris')/('milostris', 'sallelorier...</td>\n",
       "      <td>('rt', 'milostris', 'sallelorier')/('milostris...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2022-05-08</td>\n",
       "      <td>1523304001252462593</td>\n",
       "      <td>RT @Pablo86418579: Aquí el gran David Icke. Ha...</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>rt  aqui gran david icke hablando detras muros...</td>\n",
       "      <td>('rt', 'aqui')/('aqui', 'gran')/('gran', 'davi...</td>\n",
       "      <td>('rt', 'aqui', 'gran')/('aqui', 'gran', 'david...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2022-05-08</td>\n",
       "      <td>1523303980042194944</td>\n",
       "      <td>RT @Pablo86418579: Robert Kennedy jr un gran p...</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>rt  robert kennedy jr gran patriota david ikce...</td>\n",
       "      <td>('rt', 'robert')/('robert', 'kennedy')/('kenne...</td>\n",
       "      <td>('rt', 'robert', 'kennedy')/('robert', 'kenned...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2022-05-08</td>\n",
       "      <td>1523303964673986561</td>\n",
       "      <td>RT @Pablo86418579: El gran Ricardo Delgado par...</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>rt  gran ricardo delgado mundo grafeno propied...</td>\n",
       "      <td>('rt', 'gran')/('gran', 'ricardo')/('ricardo',...</td>\n",
       "      <td>('rt', 'gran', 'ricardo')/('gran', 'ricardo', ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   created_at                   id  \\\n",
       "0  2022-05-08  1523324379807072258   \n",
       "1  2022-05-08  1523309540137127937   \n",
       "2  2022-05-08  1523307982087368706   \n",
       "3  2022-05-08  1523306196689891329   \n",
       "4  2022-05-08  1523306139282448384   \n",
       "5  2022-05-08  1523305329639006208   \n",
       "6  2022-05-08  1523304162280108032   \n",
       "7  2022-05-08  1523304001252462593   \n",
       "8  2022-05-08  1523303980042194944   \n",
       "9  2022-05-08  1523303964673986561   \n",
       "\n",
       "                                                text  retweet_count  \\\n",
       "0  RT @hip0critic: @FrayJosepho \"COBIPERSISTENTE\"...             13   \n",
       "1  RT @BassanVirginia: @sembraodo2 @ignaziololo1 ...              2   \n",
       "2  Buen Domingo y descanso a todos. \\nRock Puro d...              0   \n",
       "3  RT @Pablo86418579: https://t.co/xdVHT8Mtwt\\nAc...              5   \n",
       "4  RT @sembraodo2: @ignaziololo1 terribles jesuit...              1   \n",
       "5  Hablamos de la Viruela hace 2 meses... que \"de...              7   \n",
       "6  RT @RCLatin2: @milostris @sallelorier @SciutoD...              2   \n",
       "7  RT @Pablo86418579: Aquí el gran David Icke. Ha...             13   \n",
       "8  RT @Pablo86418579: Robert Kennedy jr un gran p...             17   \n",
       "9  RT @Pablo86418579: El gran Ricardo Delgado par...             22   \n",
       "\n",
       "   like_count                                         clean_text  \\\n",
       "0           0  rt  frayjosepho cobipersistente dicese hackeo ...   \n",
       "1           0  rt bassanvirginia   crees papa francisco insta...   \n",
       "2           1  buen domingo descanso rock puro pura sangre li...   \n",
       "3           0  rt   aca luisiana eeuu aborto entiende  uribur...   \n",
       "4           0                           rt   terribles jesuitas    \n",
       "5          11  hablamos viruela  meses desaparecio vial viruz...   \n",
       "6           0  rt  milostris sallelorier sciutodr    curiosa ...   \n",
       "7           0  rt  aqui gran david icke hablando detras muros...   \n",
       "8           0  rt  robert kennedy jr gran patriota david ikce...   \n",
       "9           0  rt  gran ricardo delgado mundo grafeno propied...   \n",
       "\n",
       "                                    bigram_key_words  \\\n",
       "0  ('rt', 'frayjosepho')/('frayjosepho', 'cobiper...   \n",
       "1  ('rt', 'bassanvirginia')/('bassanvirginia', 'c...   \n",
       "2  ('buen', 'domingo')/('domingo', 'descanso')/('...   \n",
       "3  ('rt', 'aca')/('aca', 'luisiana')/('luisiana',...   \n",
       "4      ('rt', 'terribles')/('terribles', 'jesuitas')   \n",
       "5  ('hablamos', 'viruela')/('viruela', 'meses')/(...   \n",
       "6  ('rt', 'milostris')/('milostris', 'sallelorier...   \n",
       "7  ('rt', 'aqui')/('aqui', 'gran')/('gran', 'davi...   \n",
       "8  ('rt', 'robert')/('robert', 'kennedy')/('kenne...   \n",
       "9  ('rt', 'gran')/('gran', 'ricardo')/('ricardo',...   \n",
       "\n",
       "                                   trigram_key_words  \n",
       "0  ('rt', 'frayjosepho', 'cobipersistente')/('fra...  \n",
       "1  ('rt', 'bassanvirginia', 'crees')/('bassanvirg...  \n",
       "2  ('buen', 'domingo', 'descanso')/('domingo', 'd...  \n",
       "3  ('rt', 'aca', 'luisiana')/('aca', 'luisiana', ...  \n",
       "4                    ('rt', 'terribles', 'jesuitas')  \n",
       "5  ('hablamos', 'viruela', 'meses')/('viruela', '...  \n",
       "6  ('rt', 'milostris', 'sallelorier')/('milostris...  \n",
       "7  ('rt', 'aqui', 'gran')/('aqui', 'gran', 'david...  \n",
       "8  ('rt', 'robert', 'kennedy')/('robert', 'kenned...  \n",
       "9  ('rt', 'gran', 'ricardo')/('gran', 'ricardo', ...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o_df[\"created_at\"]=o_df[\"created_at\"].apply(lambda a: pd.to_datetime(a).date())\n",
    "o_df[\"clean_text\"]=o_df[\"text\"].apply(lambda x: clean_string(x))\n",
    "o_df[\"bigram_key_words\"]= o_df[\"clean_text\"].apply(lambda x: nlp_process_2(x))\n",
    "o_df[\"trigram_key_words\"]= o_df[\"clean_text\"].apply(lambda x: nlp_process_3(x))\n",
    "\n",
    "o_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MYLBfTPDCVBq"
   },
   "source": [
    "###Palabras clave por bigrama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "ddbccd12",
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bigrams</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>('rt', 'isabaelmayor')</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>('reposigustavo', 'anyjazmines')</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>('rt', 'danconiafranc')</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>('bill', 'gates')</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>('traves', 'youtube')</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>('rt', 'bravaruthless')</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>('elon', 'musk')</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>('klaus', 'schwab')</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>('reiner', 'fuellmich')</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>('rt', 'rt')</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>('efectos', 'adversos')</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>('dr', 'reiner')</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>('rt', 'hoypalestina')</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>('rt', 'sanrecargada')</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>('rt', 'patterikathy')</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>('samiiimorgan', 'mmvpdesz')</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>('banco', 'central')</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>('rt', 'claubalestrini')</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>('buen', 'dia')</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>('identidad', 'genero')</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             bigrams   0\n",
       "0             ('rt', 'isabaelmayor')  21\n",
       "1   ('reposigustavo', 'anyjazmines')  19\n",
       "2            ('rt', 'danconiafranc')  19\n",
       "3                  ('bill', 'gates')  18\n",
       "4              ('traves', 'youtube')  17\n",
       "5            ('rt', 'bravaruthless')  17\n",
       "6                   ('elon', 'musk')  16\n",
       "7                ('klaus', 'schwab')  13\n",
       "8            ('reiner', 'fuellmich')  13\n",
       "9                       ('rt', 'rt')  12\n",
       "10           ('efectos', 'adversos')  11\n",
       "11                  ('dr', 'reiner')  11\n",
       "12            ('rt', 'hoypalestina')  11\n",
       "13            ('rt', 'sanrecargada')  10\n",
       "14            ('rt', 'patterikathy')  10\n",
       "15      ('samiiimorgan', 'mmvpdesz')   9\n",
       "16              ('banco', 'central')   9\n",
       "17          ('rt', 'claubalestrini')   9\n",
       "18                   ('buen', 'dia')   9\n",
       "19           ('identidad', 'genero')   8"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key_word_bigram = o_df[o_df['clean_text']!='']['bigram_key_words'].to_list()\n",
    "all_key_word_bigram = '/'.join([str(i) for i in key_word_bigram])\n",
    "df_all_key_word_bigram = pd.DataFrame(data={'bigrams': all_key_word_bigram.split('/')})\n",
    "#DF_MOTIVO_DETRACTORES_ALL = DF_MOTIVO_DETRACTORES_ALL[DF_MOTIVO_DETRACTORES_ALL['bigrams']!=\"('buena', 'universidad')\"]\n",
    "#DF_MOTIVO_DETRACTORES_ALL = DF_MOTIVO_DETRACTORES_ALL[DF_MOTIVO_DETRACTORES_ALL['bigrams']!=\"('universidad', 'buena')\"]\n",
    "count_bigram_tweets = df_all_key_word_bigram.value_counts().reset_index()\n",
    "count_bigram_tweets = count_bigram_tweets[count_bigram_tweets[\"bigrams\"] != \"\"].reset_index(inplace=False).drop(['index'], axis=1)\n",
    "count_bigram_tweets[0:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N7qlqAzMCe3C"
   },
   "source": [
    "###Palabras clave por trigrama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "XbrhNK0b814Z"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trigrams</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>('dr', 'reiner', 'fuellmich')</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>('rt', 'rt', 'rt')</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>('catherine', 'austin', 'fitts')</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>('cilviv', 'reposigustavo', 'anyjazmines')</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>('diputados', 'panama', 'aqui')</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>('digo', 'digo', 'digo')</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>('explosivas', 'declaraciones', 'senador')</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>('rt', 'patterikathy', 'valentincbcb')</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>('panama', 'aqui', 'paso')</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>('paso', 'mismo', 'ocultaron')</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>('mismo', 'ocultaron', 'no…')</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>('aqui', 'paso', 'mismo')</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>('reiner', 'fuellmich', 'afirma')</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>('rt', 'explosivas', 'declaraciones')</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>('declaraciones', 'senador', 'australiano')</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>('australiano', 'diputados', 'panama')</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>('senador', 'australiano', 'diputados')</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>('materno', 'infantil', 'mar')</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>('mar', 'plata', 'aumento')</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>('lidocaina', 'epinefrina', 'normon')</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       trigrams   0\n",
       "0                 ('dr', 'reiner', 'fuellmich')  11\n",
       "1                            ('rt', 'rt', 'rt')   8\n",
       "2              ('catherine', 'austin', 'fitts')   7\n",
       "3    ('cilviv', 'reposigustavo', 'anyjazmines')   6\n",
       "4               ('diputados', 'panama', 'aqui')   5\n",
       "5                      ('digo', 'digo', 'digo')   5\n",
       "6    ('explosivas', 'declaraciones', 'senador')   5\n",
       "7        ('rt', 'patterikathy', 'valentincbcb')   5\n",
       "8                    ('panama', 'aqui', 'paso')   5\n",
       "9                ('paso', 'mismo', 'ocultaron')   5\n",
       "10                ('mismo', 'ocultaron', 'no…')   5\n",
       "11                    ('aqui', 'paso', 'mismo')   5\n",
       "12            ('reiner', 'fuellmich', 'afirma')   5\n",
       "13        ('rt', 'explosivas', 'declaraciones')   5\n",
       "14  ('declaraciones', 'senador', 'australiano')   5\n",
       "15       ('australiano', 'diputados', 'panama')   5\n",
       "16      ('senador', 'australiano', 'diputados')   5\n",
       "17               ('materno', 'infantil', 'mar')   4\n",
       "18                  ('mar', 'plata', 'aumento')   4\n",
       "19        ('lidocaina', 'epinefrina', 'normon')   4"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key_word_trigram = o_df[o_df['clean_text']!='']['trigram_key_words'].to_list()\n",
    "all_key_word_trigram = '/'.join([str(i) for i in key_word_trigram])\n",
    "df_all_key_word_trigram = pd.DataFrame(data={'trigrams': all_key_word_trigram.split('/')})\n",
    "\n",
    "count_trigram_tweets = df_all_key_word_trigram.value_counts().reset_index()\n",
    "count_trigram_tweets = count_trigram_tweets[count_trigram_tweets[\"trigrams\"] != \"\"].reset_index(inplace=False).drop(['index'], axis=1)\n",
    "count_trigram_tweets[0:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ayuTEBUN814Z"
   },
   "source": [
    "#### Añadir visualización (Falta desarrollar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "2Ob1ulHY814a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_at</th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>like_count</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>bigram_key_words</th>\n",
       "      <th>trigram_key_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-05-08</td>\n",
       "      <td>1523324379807072258</td>\n",
       "      <td>RT @hip0critic: @FrayJosepho \"COBIPERSISTENTE\"...</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>rt  frayjosepho cobipersistente dicese hackeo ...</td>\n",
       "      <td>('rt', 'frayjosepho')/('frayjosepho', 'cobiper...</td>\n",
       "      <td>('rt', 'frayjosepho', 'cobipersistente')/('fra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-05-08</td>\n",
       "      <td>1523309540137127937</td>\n",
       "      <td>RT @BassanVirginia: @sembraodo2 @ignaziololo1 ...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>rt bassanvirginia   crees papa francisco insta...</td>\n",
       "      <td>('rt', 'bassanvirginia')/('bassanvirginia', 'c...</td>\n",
       "      <td>('rt', 'bassanvirginia', 'crees')/('bassanvirg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-05-08</td>\n",
       "      <td>1523307982087368706</td>\n",
       "      <td>Buen Domingo y descanso a todos. \\nRock Puro d...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>buen domingo descanso rock puro pura sangre li...</td>\n",
       "      <td>('buen', 'domingo')/('domingo', 'descanso')/('...</td>\n",
       "      <td>('buen', 'domingo', 'descanso')/('domingo', 'd...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   created_at                   id  \\\n",
       "0  2022-05-08  1523324379807072258   \n",
       "1  2022-05-08  1523309540137127937   \n",
       "2  2022-05-08  1523307982087368706   \n",
       "\n",
       "                                                text  retweet_count  \\\n",
       "0  RT @hip0critic: @FrayJosepho \"COBIPERSISTENTE\"...             13   \n",
       "1  RT @BassanVirginia: @sembraodo2 @ignaziololo1 ...              2   \n",
       "2  Buen Domingo y descanso a todos. \\nRock Puro d...              0   \n",
       "\n",
       "   like_count                                         clean_text  \\\n",
       "0           0  rt  frayjosepho cobipersistente dicese hackeo ...   \n",
       "1           0  rt bassanvirginia   crees papa francisco insta...   \n",
       "2           1  buen domingo descanso rock puro pura sangre li...   \n",
       "\n",
       "                                    bigram_key_words  \\\n",
       "0  ('rt', 'frayjosepho')/('frayjosepho', 'cobiper...   \n",
       "1  ('rt', 'bassanvirginia')/('bassanvirginia', 'c...   \n",
       "2  ('buen', 'domingo')/('domingo', 'descanso')/('...   \n",
       "\n",
       "                                   trigram_key_words  \n",
       "0  ('rt', 'frayjosepho', 'cobipersistente')/('fra...  \n",
       "1  ('rt', 'bassanvirginia', 'crees')/('bassanvirg...  \n",
       "2  ('buen', 'domingo', 'descanso')/('domingo', 'd...  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "975ea8b4"
   },
   "source": [
    "# EXCEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "088bfef2"
   },
   "outputs": [],
   "source": [
    "with pd.ExcelWriter('Ngram_Analysis.xlsx') as writer:  \n",
    "    o_df.to_excel(writer, sheet_name='Base')    \n",
    "    count_trigram_tweets.to_excel(writer, sheet_name='Bigrams')\n",
    "    count_bigram_tweets.to_excel(writer, sheet_name='Trigrams')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f48ac031"
   },
   "source": [
    "## NLP categorizacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "e8c6c704"
   },
   "outputs": [],
   "source": [
    "# Create a new document-term matrix using only nouns\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ea27ed6a"
   },
   "source": [
    "## Oportunidades de Mejora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "b617633a"
   },
   "outputs": [],
   "source": [
    "o_df_nouns=o_df[[\"clean_text\"]].dropna().reset_index(inplace=False).dropna().drop('index', axis=1, inplace=False)\n",
    "\n",
    "# We are going to create a document-term matrix using CountVectorizer, and exclude common Spanish stop words\n",
    "\n",
    "lines_df = pd.read_csv('spanish.txt', sep=\"\\t\", header=None)\n",
    "lines_df.columns = [\"spanish_stopwords\"]\n",
    "lines=list(lines_df[\"spanish_stopwords\"])\n",
    "#lines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "4b18067b"
   },
   "outputs": [],
   "source": [
    "# Aplicaremos varios rounds de limpieza\n",
    "import re\n",
    "import string\n",
    "\n",
    "def clean_text_round1(text):\n",
    "    '''Make text lowercase, remove text in square brackets, remove punctuation and remove words containing numbers.'''\n",
    "    text = text.lower()\n",
    "    text = re.sub('\\[.*?¿\\]\\%', ' ', text)\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), ' ', text)\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "    return text\n",
    "\n",
    "# Segundo round\n",
    "def clean_text_round2(text):\n",
    "    '''Get rid of some additional punctuation and non-sensical text that was missed the first time around.'''\n",
    "    text = re.sub('[‘’“”…«»]', '', text)\n",
    "    text = re.sub('\\n', ' ', text)\n",
    "    return text\n",
    "'''\n",
    "# Como no tenemos un Lemmatizer en español, hacemos manualmente algunas conversiones\n",
    "# OJO: esto realmente no se hace a mano!!!\n",
    "\n",
    "def detectadas(palabra):\n",
    "    eliminar_s = ('libreros','textos','papelitos','monedas','páginas','anécdotas','perros','cuadernos','blogs',\n",
    "                  'revistas','caballos','vecinos','madres','puntos','ricos','libros')\n",
    "    if palabra in eliminar_s :\n",
    "        return palabra[:-1]\n",
    "    eliminar_es = ('mundiales','lectores','campeones','maníes','ustedes','autores')\n",
    "    if palabra in eliminar_es:\n",
    "        return palabra[:-2]\n",
    "    return palabra\n",
    "\n",
    "def clean_text_round3(text):\n",
    "    return \" \".join([detectadas(word) for word in text.split()])\n",
    "    \n",
    "'''\n",
    "round1 = lambda x: clean_text_round1(x)\n",
    "round2 = lambda x: clean_text_round2(x)\n",
    "#round3 = lambda x: clean_text_round3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "37b74ab7"
   },
   "outputs": [],
   "source": [
    "# vemos la primer limpieza\n",
    "data_clean = pd.DataFrame(o_df_nouns[\"clean_text\"].apply(round1))\n",
    "data_clean = pd.DataFrame(o_df_nouns[\"clean_text\"].apply(round2))\n",
    "#data_clean = pd.DataFrame(data_nouns[\"OportunidadesMejoraTextClean\"].apply(round3))\n",
    "\n",
    "# Esto es un nuevo campo por si quisieramos agregar alguna info adicional a cada año\n",
    "# Nuestro caso repetimos los años, nos servirá para alguna visualización\n",
    "full_names = list(data_clean.reset_index()[\"index\"])#['2004','2005','2006','2007','2008','2009','2010','2011','2012','2013','2014','2015']\n",
    "\n",
    "o_df_nouns['full_name'] = full_names# nombre de indices del comentario\n",
    "\n",
    "# Lo guardamos como pickle\n",
    "o_df_nouns.to_pickle(\"data_nouns.pkl\")\n",
    "\n",
    "cv = CountVectorizer(stop_words=lines)\n",
    "data_cv = cv.fit_transform(data_clean[\"clean_text\"])\n",
    "data_dtm = pd.DataFrame(data_cv.toarray(), columns=cv.get_feature_names())\n",
    "data_dtm.index = data_clean.index\n",
    "\n",
    "# Lo guardamos como pickle\n",
    "data_dtm.to_pickle(\"dtm.pkl\")\n",
    "\n",
    "# Lo guardamos como pickle también\n",
    "data_clean.to_pickle('data_clean.pkl')\n",
    "pickle.dump(cv, open(\"cv.pkl\", \"wb\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "fc4db607"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aaron</th>\n",
       "      <th>abajo</th>\n",
       "      <th>abandonar</th>\n",
       "      <th>abandono</th>\n",
       "      <th>abandonos</th>\n",
       "      <th>abarcan</th>\n",
       "      <th>abardein</th>\n",
       "      <th>abatidos</th>\n",
       "      <th>abducidas</th>\n",
       "      <th>abducido</th>\n",
       "      <th>...</th>\n",
       "      <th>zmesteve</th>\n",
       "      <th>zo</th>\n",
       "      <th>zombie</th>\n",
       "      <th>zombies</th>\n",
       "      <th>zombificacion</th>\n",
       "      <th>zona</th>\n",
       "      <th>zorro</th>\n",
       "      <th>zuckerberg</th>\n",
       "      <th>zurdos</th>\n",
       "      <th>ña</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2509</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2510</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2511</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2512</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2513</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2514 rows × 10258 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      aaron  abajo  abandonar  abandono  abandonos  abarcan  abardein  \\\n",
       "0         0      0          0         0          0        0         0   \n",
       "1         0      0          0         0          0        0         0   \n",
       "2         0      0          0         0          0        0         0   \n",
       "3         0      0          0         0          0        0         0   \n",
       "4         0      0          0         0          0        0         0   \n",
       "...     ...    ...        ...       ...        ...      ...       ...   \n",
       "2509      0      0          0         0          0        0         0   \n",
       "2510      0      0          0         0          0        0         0   \n",
       "2511      0      0          0         0          0        0         0   \n",
       "2512      0      0          0         0          0        0         0   \n",
       "2513      0      0          0         0          0        0         0   \n",
       "\n",
       "      abatidos  abducidas  abducido  ...  zmesteve  zo  zombie  zombies  \\\n",
       "0            0          0         0  ...         0   0       0        0   \n",
       "1            0          0         0  ...         0   0       0        0   \n",
       "2            0          0         0  ...         0   0       0        0   \n",
       "3            0          0         0  ...         0   0       0        0   \n",
       "4            0          0         0  ...         0   0       0        0   \n",
       "...        ...        ...       ...  ...       ...  ..     ...      ...   \n",
       "2509         0          0         0  ...         0   0       0        0   \n",
       "2510         0          0         0  ...         0   0       0        0   \n",
       "2511         0          0         0  ...         0   0       0        0   \n",
       "2512         0          0         0  ...         0   0       0        0   \n",
       "2513         0          0         0  ...         0   0       0        0   \n",
       "\n",
       "      zombificacion  zona  zorro  zuckerberg  zurdos  ña  \n",
       "0                 0     0      0           0       0   0  \n",
       "1                 0     0      0           0       0   0  \n",
       "2                 0     0      0           0       0   0  \n",
       "3                 0     0      0           0       0   0  \n",
       "4                 0     0      0           0       0   0  \n",
       "...             ...   ...    ...         ...     ...  ..  \n",
       "2509              0     0      0           0       0   0  \n",
       "2510              0     0      0           0       0   0  \n",
       "2511              0     0      0           0       0   0  \n",
       "2512              0     0      0           0       0   0  \n",
       "2513              0     0      0           0       0   0  \n",
       "\n",
       "[2514 rows x 10258 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in cleaned data\n",
    "data_clean = pd.read_pickle('data_clean.pkl')\n",
    "\n",
    "# Add new stop words\n",
    "lines_df = pd.read_csv('spanish.txt', sep=\"\\t\", header=None)\n",
    "lines_df.columns = [\"spanish_stopwords\"]\n",
    "stop_words=list(lines_df[\"spanish_stopwords\"])\n",
    "\n",
    "# Recreate document-term matrix\n",
    "cv = CountVectorizer(stop_words=stop_words)\n",
    "data_cv = cv.fit_transform(data_clean[\"clean_text\"])\n",
    "data_stop = pd.DataFrame(data_cv.toarray(), columns=cv.get_feature_names())\n",
    "data_stop.index = data_clean.index\n",
    "\n",
    "# Pickle it for later use\n",
    "pickle.dump(cv, open(\"cv_stop.pkl\", \"wb\"))\n",
    "data_stop.to_pickle(\"dtm_stop.pkl\")\n",
    "# Let's read in our document-term matrix\n",
    "data = pd.read_pickle('dtm_stop.pkl')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "bc3ac909"
   },
   "outputs": [],
   "source": [
    "# Import the necessary modules for LDA with gensim\n",
    "# Terminal / Anaconda Navigator: conda install -c conda-forge gensim\n",
    "from gensim import matutils, models\n",
    "import scipy.sparse\n",
    "\n",
    "# import logging\n",
    "# logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "# One of the required inputs is a term-document matrix\n",
    "tdm = data.transpose()\n",
    "#tdm.tail()\n",
    "\n",
    "# We're going to put the term-document matrix into a new gensim format, from df --> sparse matrix --> gensim corpus\n",
    "sparse_counts = scipy.sparse.csr_matrix(tdm)\n",
    "corpus = matutils.Sparse2Corpus(sparse_counts)\n",
    "\n",
    "# Gensim also requires dictionary of the all terms and their respective location in the term-document matrix\n",
    "cv = pickle.load(open(\"cv_stop.pkl\", \"rb\"))\n",
    "id2word = dict((v, k) for k, v in cv.vocabulary_.items())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "04cebedc"
   },
   "source": [
    "## Análisis de temáticas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "9dfc590a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.034*\"rt\" + 0.003*\"guerra\" + 0.003*\"asi\" + 0.003*\"ceciliamediar\" + 0.002*\"puede\" + 0.002*\"mismo\" + 0.002*\"agenda\" + 0.002*\"gente\" + 0.002*\"gracias\" + 0.002*\"estan\"'),\n",
       " (1,\n",
       "  '0.024*\"rt\" + 0.003*\"mundo\" + 0.003*\"jajajaj\" + 0.003*\"va\" + 0.003*\"solo\" + 0.002*\"ahora\" + 0.002*\"niños\" + 0.002*\"bassanvirginia\" + 0.002*\"ver\" + 0.002*\"despues\"')]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now that we have the corpus (term-document matrix) and id2word (dictionary of location: term),\n",
    "# we need to specify two other parameters as well - the number of topics and the number of passes\n",
    "lda = models.LdaModel(corpus=corpus, id2word=id2word, num_topics=2, passes=10)\n",
    "lda.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "a2643fbf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: ['rt', 'guerra', 'asi', 'ceciliamediar', 'puede', 'mismo', 'agenda', 'gente', 'gracias', 'estan']\n",
      "Topic: 1 \n",
      "Words: ['rt', 'mundo', 'jajajaj', 'va', 'solo', 'ahora', 'niños', 'bassanvirginia', 'ver', 'despues']\n"
     ]
    }
   ],
   "source": [
    "for idx, topic in lda.show_topics(formatted=False, num_words= 10):\n",
    "    print('Topic: {} \\nWords: {}'.format(idx, [w[0] for w in topic]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "777efef6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.022*\"rt\" + 0.003*\"jajajaj\" + 0.003*\"nunca\" + 0.002*\"asi\" + 0.002*\"youtube\" + 0.002*\"personas\" + 0.002*\"hoy\" + 0.002*\"ceciliamediar\" + 0.002*\"tambien\" + 0.002*\"gates\"'),\n",
       " (1,\n",
       "  '0.035*\"rt\" + 0.004*\"solo\" + 0.003*\"ahora\" + 0.003*\"años\" + 0.003*\"pais\" + 0.003*\"mujer\" + 0.003*\"gente\" + 0.003*\"andreaferreravf\" + 0.002*\"brasil\" + 0.002*\"sexo\"'),\n",
       " (2,\n",
       "  '0.030*\"rt\" + 0.003*\"va\" + 0.003*\"asi\" + 0.003*\"puede\" + 0.003*\"estan\" + 0.003*\"guerra\" + 0.003*\"mundo\" + 0.003*\"argentina\" + 0.003*\"bassanvirginia\" + 0.003*\"todas\"')]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LDA for num_topics = 3\n",
    "lda_3 = models.LdaModel(corpus=corpus, id2word=id2word, num_topics=3, passes=10)\n",
    "lda_3.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "29835161",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.036*\"rt\" + 0.003*\"hoy\" + 0.003*\"nunca\" + 0.003*\"mujer\" + 0.003*\"ver\" + 0.002*\"comun\" + 0.002*\"hacer\" + 0.002*\"años\" + 0.002*\"pueblo\" + 0.002*\"niños\"'),\n",
       " (1,\n",
       "  '0.022*\"rt\" + 0.004*\"asi\" + 0.004*\"va\" + 0.004*\"bassanvirginia\" + 0.003*\"gente\" + 0.003*\"sexo\" + 0.003*\"genero\" + 0.003*\"claro\" + 0.003*\"agenda\" + 0.003*\"mundo\"'),\n",
       " (2,\n",
       "  '0.036*\"rt\" + 0.004*\"despues\" + 0.003*\"telegram\" + 0.003*\"andreaferreravf\" + 0.003*\"pais\" + 0.003*\"solo\" + 0.003*\"youtube\" + 0.003*\"dice\" + 0.002*\"causa\" + 0.002*\"guerra\"'),\n",
       " (3,\n",
       "  '0.020*\"rt\" + 0.004*\"decir\" + 0.004*\"tambien\" + 0.003*\"puede\" + 0.003*\"nadal\" + 0.003*\"ceciliamediar\" + 0.003*\"reposigustavo\" + 0.003*\"anyjazmines\" + 0.003*\"va\" + 0.003*\"mundo\"')]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LDA for num_topics = 4\n",
    "lda_4 = models.LdaModel(corpus=corpus, id2word=id2word, num_topics=4, passes=10)\n",
    "lda_4.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "1f1a5940"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.035*\"rt\" + 0.006*\"gente\" + 0.006*\"asi\" + 0.005*\"va\" + 0.005*\"digital\" + 0.005*\"siempre\" + 0.004*\"twitter\" + 0.004*\"biden\" + 0.004*\"dia\" + 0.004*\"mal\"'),\n",
       " (1,\n",
       "  '0.037*\"rt\" + 0.006*\"mujer\" + 0.006*\"telegram\" + 0.006*\"mundo\" + 0.005*\"canosa\" + 0.005*\"ver\" + 0.005*\"todas\" + 0.004*\"padre\" + 0.004*\"argentina\" + 0.004*\"pais\"'),\n",
       " (2,\n",
       "  '0.024*\"rt\" + 0.010*\"andreaferreravf\" + 0.009*\"jajajaj\" + 0.006*\"siempre\" + 0.005*\"bluesilver\" + 0.004*\"brasil\" + 0.004*\"pase\" + 0.004*\"busca\" + 0.004*\"peor\" + 0.003*\"mendoza\"'),\n",
       " (3,\n",
       "  '0.028*\"rt\" + 0.008*\"nadal\" + 0.008*\"agua\" + 0.004*\"historia\" + 0.004*\"viene\" + 0.004*\"nuevo\" + 0.004*\"grande\" + 0.003*\"jajajaj\" + 0.003*\"denunciar\" + 0.003*\"youtube\"'),\n",
       " (4,\n",
       "  '0.043*\"rt\" + 0.006*\"va\" + 0.006*\"puede\" + 0.004*\"eeuu\" + 0.004*\"elenabercovich\" + 0.004*\"tema\" + 0.004*\"digo\" + 0.004*\"vamos\" + 0.003*\"pc\" + 0.003*\"hacer\"'),\n",
       " (5,\n",
       "  '0.044*\"rt\" + 0.005*\"mundial\" + 0.004*\"guerra\" + 0.004*\"reposigustavo\" + 0.004*\"sangre\" + 0.004*\"genero\" + 0.004*\"estan\" + 0.004*\"agenda\" + 0.004*\"tv\" + 0.004*\"sistema\"'),\n",
       " (6,\n",
       "  '0.014*\"rt\" + 0.008*\"brasil\" + 0.006*\"mundo\" + 0.005*\"bassanvirginia\" + 0.005*\"vivicanosaok\" + 0.005*\"verdad\" + 0.004*\"libertad\" + 0.004*\"bolsonaro\" + 0.004*\"dicen\" + 0.004*\"sigue\"'),\n",
       " (7,\n",
       "  '0.018*\"rt\" + 0.008*\"sexo\" + 0.006*\"varios\" + 0.006*\"traves\" + 0.005*\"hombre\" + 0.005*\"nadal\" + 0.005*\"jajajaja\" + 0.005*\"youtube\" + 0.004*\"dentro\" + 0.004*\"elenabercovich\"'),\n",
       " (8,\n",
       "  '0.017*\"rt\" + 0.007*\"horas\" + 0.006*\"bill\" + 0.005*\"solo\" + 0.005*\"ningun\" + 0.005*\"ahi\" + 0.005*\"gates\" + 0.004*\"jajajajaj\" + 0.004*\"alimentos\" + 0.004*\"djokovic\"'),\n",
       " (9,\n",
       "  '0.015*\"rt\" + 0.006*\"total\" + 0.005*\"creen\" + 0.005*\"todas\" + 0.005*\"mujer\" + 0.005*\"despues\" + 0.005*\"hombre\" + 0.005*\"globo\" + 0.004*\"pq\" + 0.004*\"jaja\"')]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LDA for num_topics = 10\n",
    "lda_10 = models.LdaModel(corpus=corpus, id2word=id2word, num_topics=10, passes=10)\n",
    "lda_10.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "da8b836d"
   },
   "source": [
    "### Ordenamiento de palabras clave por temáticas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "1d9aed68",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: ['rt', 'gente', 'asi', 'va', 'digital', 'siempre']\n",
      "Topic: 1 \n",
      "Words: ['rt', 'mujer', 'telegram', 'mundo', 'canosa', 'ver']\n",
      "Topic: 2 \n",
      "Words: ['rt', 'andreaferreravf', 'jajajaj', 'siempre', 'bluesilver', 'brasil']\n",
      "Topic: 3 \n",
      "Words: ['rt', 'nadal', 'agua', 'historia', 'viene', 'nuevo']\n",
      "Topic: 4 \n",
      "Words: ['rt', 'va', 'puede', 'eeuu', 'elenabercovich', 'tema']\n",
      "Topic: 5 \n",
      "Words: ['rt', 'mundial', 'guerra', 'reposigustavo', 'sangre', 'genero']\n",
      "Topic: 6 \n",
      "Words: ['rt', 'brasil', 'mundo', 'bassanvirginia', 'vivicanosaok', 'verdad']\n",
      "Topic: 7 \n",
      "Words: ['rt', 'sexo', 'varios', 'traves', 'hombre', 'nadal']\n",
      "Topic: 8 \n",
      "Words: ['rt', 'horas', 'bill', 'solo', 'ningun', 'ahi']\n",
      "Topic: 9 \n",
      "Words: ['rt', 'total', 'creen', 'todas', 'mujer', 'despues']\n"
     ]
    }
   ],
   "source": [
    "for idx, topic in lda_10.show_topics(formatted=False, num_words= 6):\n",
    "    print('Topic: {} \\nWords: {}'.format(idx, [w[0] for w in topic]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Hackathon.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
