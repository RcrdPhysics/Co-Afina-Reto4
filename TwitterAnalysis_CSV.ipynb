{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_sIc_vI83CW5"
      },
      "source": [
        "## Obteniendo todos los tweets de un usuario"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "zg5BuU9a3gy7"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import json\n",
        "import pandas as pd\n",
        "import tweepy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ZY8WyH504C0H"
      },
      "outputs": [],
      "source": [
        "#pip install --upgrade tweepy  # needed v.4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "k4Tv2ESo2BIw"
      },
      "outputs": [],
      "source": [
        "# LA LLAVE\n",
        "BEARER_TOKEN = \"AAAAAAAAAAAAAAAAAAAAAF02cQEAAAAAAw4Ko%2BlvaP%2FEKo4jetBjHN%2BGEpY%3D8ot7Zeg7DytI7xLRXuZO23cSWSLBIAHPMKrw9b8jebi9j7nWsI\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ubalv7VSMNkO"
      },
      "source": [
        "#Extracción de data a partir del @username.\n",
        "Basado en Tweepy v4.9.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ZV7UJqdJ5Eid"
      },
      "outputs": [],
      "source": [
        "input_user = 'ignaziololo1' #Aquí se coloca el usuario al que se quiere revisar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "06b-kj461hfr"
      },
      "outputs": [],
      "source": [
        "def extract_tweet_data(input_user):\n",
        "    \"\"\"\n",
        "    Extracts data from a Twitter account.\n",
        "\n",
        "    Parameters:\n",
        "    input_user: str\n",
        "        Twitter account to be analysed (without the @).\n",
        "\n",
        "    Returns:\n",
        "    pd.DataFrame\n",
        "        A dataframe of the user tweets containing, time of creation,\n",
        "        text, tweet id, number of retweets and number of likes.\n",
        "    \"\"\"\n",
        "    # Authorize access\n",
        "    client = tweepy.Client(BEARER_TOKEN)\n",
        "\n",
        "    # Get user ID from username\n",
        "    user_id = client.get_user(username=input_user).data.id\n",
        "\n",
        "    # Get Tweets timeline (check all pages, max 100 tweets per page, max 32 pages)\n",
        "    df_list = []\n",
        "    df_len0 = []\n",
        "    df_len = []\n",
        "    for response in tweepy.Paginator(client.get_users_tweets, user_id,\n",
        "                                    tweet_fields='created_at,lang,public_metrics',\n",
        "                                    max_results=100, limit=32):\n",
        "        page_df = pd.DataFrame.from_dict(response.data)\n",
        "        df_len0.append(len(page_df))\n",
        "        # Delete non-spanish tweets\n",
        "        page_df = page_df.drop(page_df[page_df.lang !='es'].index)\n",
        "        page_df = page_df.drop(['lang'], axis=1)\n",
        "        df_len.append(len(page_df))\n",
        "        # Organize columns\n",
        "        retweet_count = []\n",
        "        like_count = []\n",
        "        for item in page_df['public_metrics'].tolist():\n",
        "          retweet_count.append(item['retweet_count'])\n",
        "          like_count.append(item['like_count'])\n",
        "        page_df = page_df.drop(['public_metrics'], axis=1)\n",
        "        page_df['retweet_count'] = retweet_count\n",
        "        page_df['like_count'] = like_count\n",
        "        df_list.append(page_df)\n",
        "    es_tweets = round(sum(df_len)*100/sum(df_len0),1)\n",
        "    print(f'Obtained {es_tweets}% tweets in spanish from total of {sum(df_len0)} tweets from user @{input_user}.')\n",
        "    tweets_df = pd.concat(df_list, ignore_index=True)\n",
        "    return tweets_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z30Yc-sFKSIz"
      },
      "source": [
        "OLD EXTRACTION CODE (only works for obtaining user info, and only 100 tweets max)\\\n",
        "Es una modificación de los ejemplos para obtener [info del user](https://github.com/twitterdev/Twitter-API-v2-sample-code/blob/main/User-Lookup/get_users_with_bearer_token.py) y [sus tweets](https://github.com/twitterdev/Twitter-API-v2-sample-code/blob/main/User-Tweet-Timeline/user_tweets.py)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ySCOAv1-s5Je"
      },
      "outputs": [],
      "source": [
        "def create_url(username, twitter_object):\n",
        "    if twitter_object == 'user':\n",
        "      usernames = f'usernames={input_user}'\n",
        "      user_fields = 'user.fields=description,created_at,protected,public_metrics,verified'\n",
        "      url = f'https://api.twitter.com/2/users/by?{usernames}&{user_fields}'\n",
        "    elif twitter_object == 'tweet':\n",
        "      user_dict = get_json('user')\n",
        "      user_id = user_dict['data'][0]['id']\n",
        "      url = f'https://api.twitter.com/2/users/{user_id}/tweets?max_results=100'\n",
        "    else:\n",
        "      print('Error. Wrong Twitter Object.')\n",
        "      return None\n",
        "    return url\n",
        "\n",
        "\n",
        "def get_params(twitter_object):\n",
        "    if twitter_object == 'tweet':\n",
        "      params = {\"tweet.fields\": \"created_at,lang,public_metrics\"}\n",
        "    else:\n",
        "      print('Error. Wrong Twitter Object.')\n",
        "      return None\n",
        "    return params\n",
        "\n",
        "\n",
        "def bearer_oauth_user(r):\n",
        "    r.headers[\"Authorization\"] = f\"Bearer {BEARER_TOKEN}\"\n",
        "    r.headers[\"User-Agent\"] = \"v2UserLookupPython\"\n",
        "    return r\n",
        "\n",
        "\n",
        "def bearer_oauth_tweet(r):\n",
        "    r.headers[\"Authorization\"] = f\"Bearer {BEARER_TOKEN}\"\n",
        "    r.headers[\"User-Agent\"] = \"v2UserTweetsPython\"\n",
        "    return r\n",
        "\n",
        "\n",
        "def connect_to_endpoint(url, twitter_object):\n",
        "    if twitter_object == 'user':\n",
        "      response = requests.request(\"GET\", url, auth=bearer_oauth_user,)\n",
        "    elif twitter_object == 'tweet':\n",
        "      params = get_params('tweet')\n",
        "      response = requests.request(\"GET\", url, auth=bearer_oauth_tweet,params=params)\n",
        "    else:\n",
        "        print('Error. Wrong Twitter Object.')\n",
        "        return None\n",
        "    if response.status_code != 200:\n",
        "        raise Exception(f'Request returned an error: {response.status_code} {response.text}')\n",
        "    return response.json()\n",
        "\n",
        "\n",
        "def get_json(twitter_object):\n",
        "    if twitter_object == 'user':\n",
        "      url = create_url(input_user, 'user')\n",
        "      json_response = connect_to_endpoint(url, 'user')\n",
        "    elif twitter_object == 'tweet':\n",
        "      url = create_url(input_user, 'tweet')\n",
        "      json_response = connect_to_endpoint(url, 'tweet')\n",
        "    else:\n",
        "        print('Error. Wrong Twitter Object.')\n",
        "        return None\n",
        "    return json_response\n",
        "\n",
        "\n",
        "def extract_user_data(username):  # MAIN FILE\n",
        "    \"\"\"\n",
        "    Extracts data from a Twitter account.\n",
        "\n",
        "    Parameters:\n",
        "    input_user: str\n",
        "        Twitter account to be analysed (without the @).\n",
        "    Returns:\n",
        "    dict\n",
        "        A dict containing the releveant variables about the user\n",
        "    \"\"\"\n",
        "    # Get user ID from username\n",
        "    user_dict = get_json('user')\n",
        "    #user_id = user_dict['data'][0]['id']\n",
        "\n",
        "    # Get Tweets timeline\n",
        "    #tweet_dict = get_json('tweet')\n",
        "    #tweet_pd = pd.DataFrame.from_dict(tweet_dict['data'])\n",
        "    return user_dict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jzybELSiTdWI"
      },
      "source": [
        "#Información del usuario extraida del API de twitter:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QUt6St5mOAxL",
        "outputId": "491640fa-8607-4830-c8f0-7d74ca2983ad"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'data': [{'created_at': '2018-04-28T16:09:33.000Z',\n",
              "   'description': '2das NUPCIAS CON LA PAJARRACA TWITTER. \\nContra la CABAL que corrompió la Casta Política, \\nsecuestró la Medicina, al Periodismo y Libertades Individuales',\n",
              "   'id': '990261747557814272',\n",
              "   'name': 'NACHINO _OFICIAL',\n",
              "   'protected': False,\n",
              "   'public_metrics': {'followers_count': 7247,\n",
              "    'following_count': 7113,\n",
              "    'listed_count': 21,\n",
              "    'tweet_count': 43328},\n",
              "   'username': 'ignaziololo1',\n",
              "   'verified': False}]}"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "extract_user_data(input_user)  ## Información del usuario"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "id": "ffWCJ56kOoLk",
        "outputId": "2146db50-851a-430a-9aa6-f38ea8546feb"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-461dae5a5520>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mextract_tweet_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_user\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m## DataFrame con todos los tweets en español del usuario\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-5-ef5acc57a598>\u001b[0m in \u001b[0;36mextract_tweet_data\u001b[0;34m(input_user)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \"\"\"\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m# Authorize access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mclient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtweepy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mClient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBEARER_TOKEN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;31m# Get user ID from username\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'tweepy' has no attribute 'Client'"
          ]
        }
      ],
      "source": [
        "extract_tweet_data(input_user)  ## DataFrame con todos los tweets en español del usuario"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PaYmSBmS5Wb0"
      },
      "source": [
        "#Procesamiento de data\n",
        "\n",
        "Limpieza y lematizacion de texto\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C1yYEvhi8wsk"
      },
      "outputs": [],
      "source": [
        "def vars_word(word, input_df):\n",
        "    \"\"\"\n",
        "    Compute the variables for a given word.\n",
        "\n",
        "    Parameters:\n",
        "    word: str\n",
        "        Word to be analysed in the timeline tweets.\n",
        "    input_df: pd.DataFrame\n",
        "        A dataframe of the user tweets containing, time of creation,\n",
        "        text, tweet id, number of retweets and number of likes.\n",
        "\n",
        "    Returns:\n",
        "    tuple(float, float, float, float, float):\n",
        "        A tuple containing variables of the word: frequency, like rate, retweet\n",
        "        rate, popularity, and polemicity\n",
        "    tuple(float, float, float):\n",
        "        A tuple containing counters of the word: number of times it appears\n",
        "        on all the tweets, number of tweets it appears in, number of retweets it\n",
        "        appears in.\n",
        "    \"\"\"\n",
        "\n",
        "    df = input_df.copy()\n",
        "    # Flatten text column\n",
        "    splitted_tweets=df.clean_text.apply(lambda a: a.split()).tolist()#.count(\"datos\")\n",
        "    # Flatten text column\n",
        "    total_word_list = [word_tweet for word_list in splitted_tweets for word_tweet in word_list]\n",
        "    \n",
        "\n",
        "    word_count = total_word_list.count(word)\n",
        "    total_word_count = len(total_word_list)\n",
        "    freq = word_count/total_word_count\n",
        "\n",
        "    df['word_count'] = df.clean_text.map(lambda t: t.count(word))\n",
        "    tweets_containing = df['word_count'].astype(bool).sum()\n",
        "    #if tweets_containing == 0:\n",
        "    #    return None\n",
        "    df.drop(['word_count'], axis=1)\n",
        "    # like_count_flag\n",
        "    like_count = df[df['word_count'].astype(bool)]['like_count'].sum()\n",
        "    retweet_count = df[df['word_count'].astype(bool)]['retweet_count'].sum()\n",
        "    #if (like_count == 0) or (retweet_count == 0):\n",
        "    #    return None\n",
        "    like_rate = like_count/tweets_containing\n",
        "    retweet_rate = retweet_count/tweets_containing\n",
        "\n",
        "    popularity  = retweet_rate/freq\n",
        "    polemicity  = retweet_rate/like_rate\n",
        "\n",
        "    variables = (freq, like_rate, retweet_rate, popularity, polemicity)\n",
        "    counters = (word_count, like_count, retweet_count)\n",
        "    return variables, counters\n",
        "\n",
        "\n",
        "def vars_cat(category, df):\n",
        "    \"\"\"\n",
        "    Compute the variables for a given category.\n",
        "\n",
        "    Parameters:\n",
        "    category: list\n",
        "        List of words to be analysed in group in the timeline tweets.\n",
        "    df: pd.DataFrame\n",
        "        A dataframe of the user tweets containing, time of creation,\n",
        "        text, tweet id, number of retweets and number of likes.\n",
        "\n",
        "    Returns:\n",
        "    tuple(float, float, float):\n",
        "        A tuple containing counters of the words in the category: number of\n",
        "        times they appear on all the tweets, number of tweets they appears in,\n",
        "        number of retweets they appear in.\n",
        "    tuple(float, float, float):\n",
        "        A tuple containing popularity and polemicity of the category.\n",
        "    \"\"\"\n",
        "\n",
        "    category = [word for word in category if word]\n",
        "    pop_cat = pol_cat = word_count_cat = like_count_cat = retweet_count_cat = 0\n",
        "    for word in category:\n",
        "        #if vars_word(word, df) is None:\n",
        "        vars, counters = vars_word(word, df)\n",
        "        pop_cat += counters[0] * vars[3]\n",
        "        pol_cat += counters[0] * vars[4]\n",
        "        word_count_cat += counters[0]\n",
        "        like_count_cat += counters[1]\n",
        "        retweet_count_cat += counters[2]\n",
        "\n",
        "    pop_cat = pop_cat / word_count_cat\n",
        "    pol_cat = pol_cat / word_count_cat\n",
        "\n",
        "    cat_counters = (word_count_cat, like_count_cat, retweet_count_cat)\n",
        "    pop_pol_cats = (pop_cat, pol_cat)\n",
        "    return cat_counters, pop_pol_cats"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PGZeprWKIO0m"
      },
      "source": [
        "## EDA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zEuHzoym814S"
      },
      "outputs": [],
      "source": [
        "import itertools\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "import re\n",
        "import string\n",
        "from bs4 import BeautifulSoup\n",
        "import nltk\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "import spacy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Y6zxw06814T"
      },
      "outputs": [],
      "source": [
        "# Load spacy\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "def clean_string(text, stem=\"None\"):\n",
        "    try:\n",
        "\n",
        "        final_string = \"\"\n",
        "\n",
        "        # minusculas\n",
        "        text = text.lower()\n",
        "        text = text.replace(\" u \",\" \").replace(\"á\",\"a\").replace(\"é\",\"e\").replace(\"í\",\"i\").replace(\"ó\",\"o\").replace(\"ú\",\"u\")\n",
        "\n",
        "        # quitar salto de lineas\n",
        "        emoji_pattern = re.compile(\"[\"\n",
        "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "        u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
        "        u\"\\U00002702-\\U000027B0\"\n",
        "        u\"\\U00002702-\\U000027B0\"\n",
        "        u\"\\U000024C2-\\U0001F251\"\n",
        "        u\"\\U0001f926-\\U0001f937\"\n",
        "        u\"\\U00010000-\\U0010ffff\"\n",
        "        u\"\\u2640-\\u2642\" \n",
        "        u\"\\u2600-\\u2B55\"\n",
        "        u\"\\u200d\"\n",
        "        u\"\\u23cf\"\n",
        "        u\"\\u23e9\"\n",
        "        u\"\\u231a\"\n",
        "        u\"\\ufe0f\"  # dingbats\n",
        "        u\"\\u3030\"\n",
        "                      \"]+\", re.UNICODE)\n",
        "        text=emoji_pattern.sub(r'', text)\n",
        "        # quitar signos gramaticales\n",
        "        translator = str.maketrans('', '', string.punctuation)\n",
        "        text = text.translate(translator)\n",
        "\n",
        "        # stopwords\n",
        "        text = text.split()\n",
        "        useless_words = nltk.corpus.stopwords.words(\"spanish\")\n",
        "        useless_words = useless_words + ['...','considero','tal','vez','seria','debe','tener','siento', \"dar\", \"hacer\"\n",
        "                            'estan','tan','parece','ademas','debido','cuenta','hace','cada','toda','si', \"ser\",\n",
        "                                         \"ma\", \"mas\", \"más\",\"bien\", \"buena\", \"creo\", \"aun\"]\n",
        "        # \n",
        "        text_filtered = [word for word in text if not word in useless_words]\n",
        "\n",
        "        # quitar numeros\n",
        "        text_filtered = [re.sub(r'\\w*\\d\\w*', '', w) for w in text_filtered]\n",
        "        \n",
        "\n",
        "        # Lematizacion\n",
        "        #if stem == 'Stem':\n",
        "        #    stemmer = PorterStemmer() \n",
        "        #    text_stemmed = [stemmer.stem(y) for y in text_filtered]\n",
        "        #elif stem == 'Lem':\n",
        "        #    lem = WordNetLemmatizer()\n",
        "        #    text_stemmed = [lem.lemmatize(y) for y in text_filtered]\n",
        "        #elif stem == 'Spacy':\n",
        "        #    text_filtered = nlp(' '.join(text_filtered))\n",
        "        #    text_stemmed = [y.lemma_ for y in text_filtered]\n",
        "        #else:\n",
        "        #    text_stemmed = text_filtered\n",
        "\n",
        "        #final_string = ' '.join(text_stemmed)\n",
        "        final_string = ' '.join(text_filtered)\n",
        "\n",
        "        # sinonimia\n",
        "        #final_string = final_string.replace('docent','profesor')\n",
        "    except AttributeError:\n",
        "        final_string = \"\"\n",
        "\n",
        "    return final_string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "noSa2PuL814U"
      },
      "outputs": [],
      "source": [
        "def ngrams_wrapper_1(sent):\n",
        "    return list(nltk.ngrams(sent, 1))\n",
        "\n",
        "def ngrams_wrapper_2(sent):\n",
        "    return list(nltk.ngrams(sent, 2))\n",
        "\n",
        "def ngrams_wrapper_3(sent):\n",
        "    return list(nltk.ngrams(sent, 3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NiYtEQ4M814V"
      },
      "outputs": [],
      "source": [
        "def make_list_bigrams_1(text):\n",
        "    sentences = nltk.sent_tokenize(text)\n",
        "    tokenized = map(nltk.tokenize.word_tokenize, sentences)\n",
        "    bigrams = map(ngrams_wrapper_1, tokenized)\n",
        "    bigram = list(itertools.chain.from_iterable(bigrams))\n",
        "    return bigram\n",
        "\n",
        "def make_list_bigrams_2(text):\n",
        "    sentences = nltk.sent_tokenize(text)\n",
        "    tokenized = map(nltk.tokenize.word_tokenize, sentences)\n",
        "    bigrams = map(ngrams_wrapper_2, tokenized)\n",
        "    bigram = list(itertools.chain.from_iterable(bigrams))\n",
        "    return bigram\n",
        "\n",
        "def make_list_bigrams_3(text):\n",
        "    sentences = nltk.sent_tokenize(text)\n",
        "    tokenized = map(nltk.tokenize.word_tokenize, sentences)\n",
        "    bigrams = map(ngrams_wrapper_3, tokenized)\n",
        "    bigram = list(itertools.chain.from_iterable(bigrams))\n",
        "    return bigram"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ptXVujDQ814W"
      },
      "outputs": [],
      "source": [
        "def nlp_process_1(text):\n",
        "    return '/'.join([str(i) for i in make_list_bigrams_1(clean_string(str(text), stem='Stem'))])\n",
        "\n",
        "def nlp_process_2(text):\n",
        "    return '/'.join([str(i) for i in make_list_bigrams_2(clean_string(str(text), stem='Stem'))])\n",
        "\n",
        "def nlp_process_3(text):\n",
        "    return '/'.join([str(i) for i in make_list_bigrams_3(clean_string(str(text), stem='Stem'))])\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "622fc216"
      },
      "source": [
        "# Subida de los tweets del usuario"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i6F1GAme814W"
      },
      "outputs": [],
      "source": [
        "# dummy input (after cleaning)\n",
        "o_df = extract_tweet_data(input_user)\n",
        "#test_df = o_df.copy()\n",
        "o_df"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Limpieza de los tweets"
      ],
      "metadata": {
        "id": "JeXTv7IJ_sUV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rcUFOzhC814X"
      },
      "outputs": [],
      "source": [
        "o_df[\"created_at\"]=o_df[\"created_at\"].apply(lambda a: pd.to_datetime(a).date())\n",
        "o_df[\"clean_text\"]=o_df[\"text\"].apply(lambda x: clean_string(x))\n",
        "o_df[\"bigram_key_words\"]= o_df[\"clean_text\"].apply(lambda x: nlp_process_2(x))\n",
        "o_df[\"trigram_key_words\"]= o_df[\"clean_text\"].apply(lambda x: nlp_process_3(x))\n",
        "\n",
        "o_df.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Palabras clave por bigrama"
      ],
      "metadata": {
        "id": "MYLBfTPDCVBq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "ddbccd12"
      },
      "outputs": [],
      "source": [
        "key_word_bigram = o_df[o_df['clean_text']!='']['bigram_key_words'].to_list()\n",
        "all_key_word_bigram = '/'.join([str(i) for i in key_word_bigram])\n",
        "df_all_key_word_bigram = pd.DataFrame(data={'bigrams': all_key_word_bigram.split('/')})\n",
        "#DF_MOTIVO_DETRACTORES_ALL = DF_MOTIVO_DETRACTORES_ALL[DF_MOTIVO_DETRACTORES_ALL['bigrams']!=\"('buena', 'universidad')\"]\n",
        "#DF_MOTIVO_DETRACTORES_ALL = DF_MOTIVO_DETRACTORES_ALL[DF_MOTIVO_DETRACTORES_ALL['bigrams']!=\"('universidad', 'buena')\"]\n",
        "count_bigram_tweets = df_all_key_word_bigram.value_counts().reset_index()\n",
        "count_bigram_tweets = count_bigram_tweets[count_bigram_tweets[\"bigrams\"] != \"\"].reset_index(inplace=False).drop(['index'], axis=1)\n",
        "count_bigram_tweets[0:20]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Palabras clave por trigrama"
      ],
      "metadata": {
        "id": "N7qlqAzMCe3C"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XbrhNK0b814Z"
      },
      "outputs": [],
      "source": [
        "key_word_trigram = o_df[o_df['clean_text']!='']['trigram_key_words'].to_list()\n",
        "all_key_word_trigram = '/'.join([str(i) for i in key_word_trigram])\n",
        "df_all_key_word_trigram = pd.DataFrame(data={'trigrams': all_key_word_trigram.split('/')})\n",
        "\n",
        "count_trigram_tweets = df_all_key_word_trigram.value_counts().reset_index()\n",
        "count_trigram_tweets = count_trigram_tweets[count_trigram_tweets[\"trigrams\"] != \"\"].reset_index(inplace=False).drop(['index'], axis=1)\n",
        "count_trigram_tweets[0:20]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ayuTEBUN814Z"
      },
      "source": [
        "#### Añadir visualización (Falta desarrollar)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Ob1ulHY814a"
      },
      "outputs": [],
      "source": [
        "o_df.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "975ea8b4"
      },
      "source": [
        "# EXCEL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "088bfef2"
      },
      "outputs": [],
      "source": [
        "with pd.ExcelWriter('Ngram_Analysis.xlsx') as writer:  \n",
        "    o_df.to_excel(writer, sheet_name='Base')    \n",
        "    count_trigram_tweets.to_excel(writer, sheet_name='Bigrams')\n",
        "    count_bigram_tweets.to_excel(writer, sheet_name='Trigrams')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f48ac031"
      },
      "source": [
        "## NLP categorizacion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e8c6c704"
      },
      "outputs": [],
      "source": [
        "# Create a new document-term matrix using only nouns\n",
        "from sklearn.feature_extraction import text\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ea27ed6a"
      },
      "source": [
        "## Oportunidades de Mejora"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b617633a"
      },
      "outputs": [],
      "source": [
        "o_df_nouns=o_df[[\"clean_text\"]].dropna().reset_index(inplace=False).dropna().drop('index', axis=1, inplace=False)\n",
        "\n",
        "# We are going to create a document-term matrix using CountVectorizer, and exclude common Spanish stop words\n",
        "\n",
        "lines_df = pd.read_csv('spanish.txt', sep=\"\\t\", header=None)\n",
        "lines_df.columns = [\"spanish_stopwords\"]\n",
        "lines=list(lines_df[\"spanish_stopwords\"])\n",
        "#lines\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4b18067b"
      },
      "outputs": [],
      "source": [
        "# Aplicaremos varios rounds de limpieza\n",
        "import re\n",
        "import string\n",
        "\n",
        "def clean_text_round1(text):\n",
        "    '''Make text lowercase, remove text in square brackets, remove punctuation and remove words containing numbers.'''\n",
        "    text = text.lower()\n",
        "    text = re.sub('\\[.*?¿\\]\\%', ' ', text)\n",
        "    text = re.sub('[%s]' % re.escape(string.punctuation), ' ', text)\n",
        "    text = re.sub('\\w*\\d\\w*', '', text)\n",
        "    return text\n",
        "\n",
        "# Segundo round\n",
        "def clean_text_round2(text):\n",
        "    '''Get rid of some additional punctuation and non-sensical text that was missed the first time around.'''\n",
        "    text = re.sub('[‘’“”…«»]', '', text)\n",
        "    text = re.sub('\\n', ' ', text)\n",
        "    return text\n",
        "'''\n",
        "# Como no tenemos un Lemmatizer en español, hacemos manualmente algunas conversiones\n",
        "# OJO: esto realmente no se hace a mano!!!\n",
        "\n",
        "def detectadas(palabra):\n",
        "    eliminar_s = ('libreros','textos','papelitos','monedas','páginas','anécdotas','perros','cuadernos','blogs',\n",
        "                  'revistas','caballos','vecinos','madres','puntos','ricos','libros')\n",
        "    if palabra in eliminar_s :\n",
        "        return palabra[:-1]\n",
        "    eliminar_es = ('mundiales','lectores','campeones','maníes','ustedes','autores')\n",
        "    if palabra in eliminar_es:\n",
        "        return palabra[:-2]\n",
        "    return palabra\n",
        "\n",
        "def clean_text_round3(text):\n",
        "    return \" \".join([detectadas(word) for word in text.split()])\n",
        "    \n",
        "'''\n",
        "round1 = lambda x: clean_text_round1(x)\n",
        "round2 = lambda x: clean_text_round2(x)\n",
        "#round3 = lambda x: clean_text_round3(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "37b74ab7"
      },
      "outputs": [],
      "source": [
        "# vemos la primer limpieza\n",
        "data_clean = pd.DataFrame(o_df_nouns[\"clean_text\"].apply(round1))\n",
        "data_clean = pd.DataFrame(o_df_nouns[\"clean_text\"].apply(round2))\n",
        "#data_clean = pd.DataFrame(data_nouns[\"OportunidadesMejoraTextClean\"].apply(round3))\n",
        "\n",
        "# Esto es un nuevo campo por si quisieramos agregar alguna info adicional a cada año\n",
        "# Nuestro caso repetimos los años, nos servirá para alguna visualización\n",
        "full_names = list(data_clean.reset_index()[\"index\"])#['2004','2005','2006','2007','2008','2009','2010','2011','2012','2013','2014','2015']\n",
        "\n",
        "o_df_nouns['full_name'] = full_names# nombre de indices del comentario\n",
        "\n",
        "# Lo guardamos como pickle\n",
        "o_df_nouns.to_pickle(\"data_nouns.pkl\")\n",
        "\n",
        "cv = CountVectorizer(stop_words=lines)\n",
        "data_cv = cv.fit_transform(data_clean[\"clean_text\"])\n",
        "data_dtm = pd.DataFrame(data_cv.toarray(), columns=cv.get_feature_names())\n",
        "data_dtm.index = data_clean.index\n",
        "\n",
        "# Lo guardamos como pickle\n",
        "data_dtm.to_pickle(\"dtm.pkl\")\n",
        "\n",
        "# Lo guardamos como pickle también\n",
        "data_clean.to_pickle('data_clean.pkl')\n",
        "pickle.dump(cv, open(\"cv.pkl\", \"wb\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fc4db607"
      },
      "outputs": [],
      "source": [
        "# Read in cleaned data\n",
        "data_clean = pd.read_pickle('data_clean.pkl')\n",
        "\n",
        "# Add new stop words\n",
        "lines_df = pd.read_csv('spanish.txt', sep=\"\\t\", header=None)\n",
        "lines_df.columns = [\"spanish_stopwords\"]\n",
        "stop_words=list(lines_df[\"spanish_stopwords\"])\n",
        "\n",
        "# Recreate document-term matrix\n",
        "cv = CountVectorizer(stop_words=stop_words)\n",
        "data_cv = cv.fit_transform(data_clean[\"clean_text\"])\n",
        "data_stop = pd.DataFrame(data_cv.toarray(), columns=cv.get_feature_names())\n",
        "data_stop.index = data_clean.index\n",
        "\n",
        "# Pickle it for later use\n",
        "pickle.dump(cv, open(\"cv_stop.pkl\", \"wb\"))\n",
        "data_stop.to_pickle(\"dtm_stop.pkl\")\n",
        "# Let's read in our document-term matrix\n",
        "data = pd.read_pickle('dtm_stop.pkl')\n",
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bc3ac909"
      },
      "outputs": [],
      "source": [
        "# Import the necessary modules for LDA with gensim\n",
        "# Terminal / Anaconda Navigator: conda install -c conda-forge gensim\n",
        "from gensim import matutils, models\n",
        "import scipy.sparse\n",
        "\n",
        "# import logging\n",
        "# logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
        "# One of the required inputs is a term-document matrix\n",
        "tdm = data.transpose()\n",
        "#tdm.tail()\n",
        "\n",
        "# We're going to put the term-document matrix into a new gensim format, from df --> sparse matrix --> gensim corpus\n",
        "sparse_counts = scipy.sparse.csr_matrix(tdm)\n",
        "corpus = matutils.Sparse2Corpus(sparse_counts)\n",
        "\n",
        "# Gensim also requires dictionary of the all terms and their respective location in the term-document matrix\n",
        "cv = pickle.load(open(\"cv_stop.pkl\", \"rb\"))\n",
        "id2word = dict((v, k) for k, v in cv.vocabulary_.items())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "04cebedc"
      },
      "source": [
        "## Análisis de temáticas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9dfc590a"
      },
      "outputs": [],
      "source": [
        "# Now that we have the corpus (term-document matrix) and id2word (dictionary of location: term),\n",
        "# we need to specify two other parameters as well - the number of topics and the number of passes\n",
        "lda = models.LdaModel(corpus=corpus, id2word=id2word, num_topics=2, passes=10)\n",
        "lda.print_topics()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a2643fbf"
      },
      "outputs": [],
      "source": [
        "for idx, topic in lda.show_topics(formatted=False, num_words= 10):\n",
        "    print('Topic: {} \\nWords: {}'.format(idx, [w[0] for w in topic]))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "777efef6"
      },
      "outputs": [],
      "source": [
        "# LDA for num_topics = 3\n",
        "lda_3 = models.LdaModel(corpus=corpus, id2word=id2word, num_topics=3, passes=10)\n",
        "lda_3.print_topics()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "29835161"
      },
      "outputs": [],
      "source": [
        "# LDA for num_topics = 4\n",
        "lda_4 = models.LdaModel(corpus=corpus, id2word=id2word, num_topics=4, passes=10)\n",
        "lda_4.print_topics()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1f1a5940"
      },
      "outputs": [],
      "source": [
        "# LDA for num_topics = 10\n",
        "lda_10 = models.LdaModel(corpus=corpus, id2word=id2word, num_topics=10, passes=10)\n",
        "lda_10.print_topics()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "da8b836d"
      },
      "source": [
        "### Ordenamiento de palabras clave por temáticas:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "1d9aed68"
      },
      "outputs": [],
      "source": [
        "for idx, topic in lda_10.show_topics(formatted=False, num_words= 6):\n",
        "    print('Topic: {} \\nWords: {}'.format(idx, [w[0] for w in topic]))\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Hackathon.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}